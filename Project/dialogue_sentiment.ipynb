{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Idea in General\n",
    "\n",
    "We have a preconcieved notion of what families or communities of people are like, in regards to their temper or sentiment they give off. We now want to investigate if it holds up to a more robust and \"objective\" analysis of these communities' sentiments. A network describing each person's relation to another based on costarring in a scene will also include a color attribute that tells us if the person is more negative or positive based on their dialogue throughout the series.  \n",
    "As such we will see clusterings of communities and can gather their coloring to determine their overall sentiments. A non visualized attribute will be the birthing place of the individual characters/nodes, maybe there's just a place that breeds negativity?  \n",
    "In addition, we will present the 5 biggest families and their sentiments as to also hold them against our previous beliefs.  \n",
    "\n",
    "We also want to analyse which persons have changed their sentiment the most throughout\n",
    "\n",
    "- Other text analysis? Most used words by each family/community?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: #fc03d7'> Steps to take </span>\n",
    "\n",
    "\n",
    "1. <span style='color: #f2d052'> Import NLP libraries </span>\n",
    "2. <span style='color: #f2d052'> Analyze sentiment on each piece of dialogue and record it into a dataframe with the speaker </span>\n",
    "3. <span style='color: #f2d052'> GroupBy speaker and average the sentiment scores of each speaker into a single number (in a new dataframe probably) </span>\n",
    "4. <span style='color: #f2d052'> Make it a little dataset for Kristi's network </span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netwulf as nw\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import random as random \n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>waymar royce</td>\n",
       "      <td>What do you expect? They're savages. One lot steals a goat from another lot and before you know it, they're ripping each other to pieces.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will</td>\n",
       "      <td>I've never seen wildlings do a thing like this. I've never seen a thing like this, not ever in my life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>waymar royce</td>\n",
       "      <td>How close did you get?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>will</td>\n",
       "      <td>Close as any man would.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gared</td>\n",
       "      <td>We should head back to the wall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23906</th>\n",
       "      <td>brienne</td>\n",
       "      <td>I think we can all agree that ships take precedence over brothels.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23907</th>\n",
       "      <td>bronn</td>\n",
       "      <td>I think that's a very presumptuous statement.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23908</th>\n",
       "      <td>tyrion lannister</td>\n",
       "      <td>I once brought a jackass and a honeycomb into a brothel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23909</th>\n",
       "      <td>man</td>\n",
       "      <td>The Queen in the North!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23910</th>\n",
       "      <td>all</td>\n",
       "      <td>The Queen in the North! The Queen in the North! The Queen in the North! The Queen in the North! The Queen in the North!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23911 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name   \n",
       "0          waymar royce  \\\n",
       "1                  will   \n",
       "2          waymar royce   \n",
       "3                  will   \n",
       "4                 gared   \n",
       "...                 ...   \n",
       "23906           brienne   \n",
       "23907             bronn   \n",
       "23908  tyrion lannister   \n",
       "23909               man   \n",
       "23910               all   \n",
       "\n",
       "                                                                                                                                        Sentence  \n",
       "0      What do you expect? They're savages. One lot steals a goat from another lot and before you know it, they're ripping each other to pieces.  \n",
       "1                                        I've never seen wildlings do a thing like this. I've never seen a thing like this, not ever in my life.  \n",
       "2                                                                                                                         How close did you get?  \n",
       "3                                                                                                                        Close as any man would.  \n",
       "4                                                                                                               We should head back to the wall.  \n",
       "...                                                                                                                                          ...  \n",
       "23906                                                                         I think we can all agree that ships take precedence over brothels.  \n",
       "23907                                                                                              I think that's a very presumptuous statement.  \n",
       "23908                                                                                   I once brought a jackass and a honeycomb into a brothel.  \n",
       "23909                                                                                                                    The Queen in the North!  \n",
       "23910                    The Queen in the North! The Queen in the North! The Queen in the North! The Queen in the North! The Queen in the North!  \n",
       "\n",
       "[23911 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform sentiment analysis on dialogue\n",
    "script = pd.read_csv('./data/Game_of_Thrones_Script.csv')\n",
    "script = script[['Name', 'Sentence']]\n",
    "script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the sentences for use in sentiment analysis on each sentence\n",
    "\n",
    "puncs = '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~' # ' is removed\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(f\"[{re.escape(puncs)}]+\", '', sentence) # remove punctuation, except apostrophe\n",
    "    sentence = re.sub(r'\\d+', '', sentence) # remove numbers\n",
    "    # remove stopwords from sentence\n",
    "    sentence = ' '.join([word for word in sentence.split() if word not in stopwords.words('english')])\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "not used"
    ]
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#! DONT TOUCH THIS CELL\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# 1000 most common words in the sentences\u001b[39;00m\n\u001b[0;32m      3\u001b[0m sent_filtered \u001b[39m=\u001b[39m script\u001b[39m.\u001b[39mSentence\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcat(sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m sent_filtered \u001b[39m=\u001b[39m preprocess(sent_filtered)\n\u001b[0;32m      7\u001b[0m sent_filtered \u001b[39m=\u001b[39m sent_filtered\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(sent_filtered[:\u001b[39m10\u001b[39m])\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      8\u001b[0m sentence \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, sentence) \u001b[39m# remove numbers\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# remove stopwords from sentence\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[0;32m     11\u001b[0m sentence \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m sentence\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m sentence \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, sentence) \u001b[39m# remove numbers\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# remove stopwords from sentence\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m)])\n\u001b[0;32m     11\u001b[0m sentence \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m sentence\n",
      "File \u001b[1;32mc:\\Users\\Simon\\OneDrive\\DTU\\8.sem\\CompScoSci\\compsoc\\.venv\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Simon\\OneDrive\\DTU\\8.sem\\CompScoSci\\compsoc\\.venv\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[39m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(f) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[39m.\u001b[39mappend(fp\u001b[39m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Users\\Simon\\OneDrive\\DTU\\8.sem\\CompScoSci\\compsoc\\.venv\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root\u001b[39m.\u001b[39;49mjoin(file)\u001b[39m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Users\\Simon\\OneDrive\\DTU\\8.sem\\CompScoSci\\compsoc\\.venv\\lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\u001b[39mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32mc:\\Users\\Simon\\OneDrive\\DTU\\8.sem\\CompScoSci\\compsoc\\.venv\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decorator\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[39m=\u001b[39m (args[\u001b[39m0\u001b[39m], add_py3_data(args[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m args[\u001b[39m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m init_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Simon\\OneDrive\\DTU\\8.sem\\CompScoSci\\compsoc\\.venv\\lib\\site-packages\\nltk\\data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[39m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mexists(_path):\n\u001b[0;32m    312\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m _path\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     20\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#! DONT TOUCH THIS CELL\n",
    "# 1000 most common words in the sentences\n",
    "sent_filtered = script.Sentence.str.cat(sep=' ')\n",
    "\n",
    "sent_filtered = preprocess(sent_filtered)\n",
    "\n",
    "sent_filtered = sent_filtered.split(' ')\n",
    "print(sent_filtered[:10])\n",
    "\n",
    "Common1000Words = Counter(sent_filtered).most_common(1000)\n",
    "Common1000Words = [word for word, count in Common1000Words]\n",
    "# Common1000Words = [word.capitalize() for word in Common1000Words]\n",
    "\n",
    "print(f\"20 most common words {Common1000Words[:20]}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most common words after removing stopwords [(' ', 288033), ('e', 139029), ('n', 73010), ('r', 69819), ('h', 65183), ('l', 46757), ('u', 38845), ('.', 34467), ('w', 26659), ('g', 23334), ('f', 21063), ('c', 18296), ('b', 14566), (\"'\", 14435), ('k', 13019), ('I', 12783), ('p', 12289), (',', 11614), ('v', 11589), ('?', 6861)]\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sent_filtered = [word for word in sent_filtered if word not in stop_words]\n",
    "print(f\"20 most common words after removing stopwords {Counter(sent_filtered).most_common(20)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: pink'> Huggingface model use </span>\n",
    "We gonna steal a sentimentmodel from huggingface which is good, but optimized to run on less resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Simon\\OneDrive\\DTU\\8.sem\\CompScoSci\\compsoc\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# skal også lige downloade torch og transformers\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "# men GPT4 har allerede givet et godt svar så skriver lige det ned xD\n",
    "# kan du downloade torch og transformers i konsolen? Og bagefter update requirements.txt med \"pip freeze > requirements.txt\" :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# SST is stanford sentiment treebank, this light version of BERT has been fine-tuned on this dataset.\n",
    "\n",
    "def analyze_continuous_sentiment(text):\n",
    "    \"\"\"\n",
    "    takes raw text as input and returns a sentiment score between -1 and 1\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    \n",
    "    # Calculate the sentiment score by subtracting the negative probability from the positive probability\n",
    "    sentiment_score = probabilities[0, 1] - probabilities[0, 0]\n",
    "    \n",
    "    # Convert the score to float and return it\n",
    "    return sentiment_score.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment of each sentence in a new column named \"sentiment_score\"\n",
    "script['sentiment_score'] = script.Sentence.iloc[0:100].apply(analyze_continuous_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bran stark</td>\n",
       "      <td>-0.476492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cassel</td>\n",
       "      <td>-0.299343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>catelyn stark</td>\n",
       "      <td>-0.101070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cersei lannister</td>\n",
       "      <td>0.497387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eddard stark</td>\n",
       "      <td>0.061471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gared</td>\n",
       "      <td>0.499138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jaime lannister</td>\n",
       "      <td>-0.561357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jon snow</td>\n",
       "      <td>0.336135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>luwin</td>\n",
       "      <td>-0.980625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>maester luwin</td>\n",
       "      <td>-0.061113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>robb stark</td>\n",
       "      <td>-0.474282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>royce</td>\n",
       "      <td>-0.973678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sansa stark</td>\n",
       "      <td>0.999692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>septa mordane</td>\n",
       "      <td>0.999725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>theon greyjoy</td>\n",
       "      <td>-0.281673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>waymar royce</td>\n",
       "      <td>-0.741911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>will</td>\n",
       "      <td>0.390046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name  sentiment_score\n",
       "0         bran stark        -0.476492\n",
       "1             cassel        -0.299343\n",
       "2      catelyn stark        -0.101070\n",
       "3   cersei lannister         0.497387\n",
       "4       eddard stark         0.061471\n",
       "5              gared         0.499138\n",
       "6    jaime lannister        -0.561357\n",
       "7           jon snow         0.336135\n",
       "8              luwin        -0.980625\n",
       "9      maester luwin        -0.061113\n",
       "10        robb stark        -0.474282\n",
       "11             royce        -0.973678\n",
       "12       sansa stark         0.999692\n",
       "13     septa mordane         0.999725\n",
       "14     theon greyjoy        -0.281673\n",
       "15      waymar royce        -0.741911\n",
       "16              will         0.390046"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each character, calculate the average sentiment score for all their sentences\n",
    "person_sentiment_scores = script.iloc[0:100].groupby('Name').sentiment_score.mean().reset_index()\n",
    "person_sentiment_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrections to the script dataset\n",
    "line 25: changed name from jonrobb to jon snow\n",
    "\n",
    "rename \"nan\" to something that doesnt get read as a \"NaN\" value...\n",
    "\n",
    "Nicolaj will go through character list and we will purge double names and the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_names = [name for name in script.Name.dropna().unique() if len(name.split(' ')) == 1]\n",
    "\n",
    "double_names = [name for name in script.Name.dropna().unique() if len(name.split(' ')) == 2]\n",
    "\n",
    "triple_names = [name for name in script.Name.dropna().unique() if len(name.split(' ')) == 3]\n",
    "\n",
    "quadruple_names = [name for name in script.Name.dropna().unique() if len(name.split(' ')) == 4]\n",
    "\n",
    "# [print(name) for name in script.Name.unique() if type(name) == float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_names = pd.DataFrame(columns=['single_name', 'count', 'in_double_name'])\n",
    "row = 0\n",
    "for name in single_names:\n",
    "    for d_name in double_names:\n",
    "        if name in d_name. split(' ')[0]:\n",
    "            count = len(script.Name[script.Name == name])\n",
    "            duplicate_names.loc[row] = [name, count, d_name]\n",
    "            row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of nouns that are not names and remove the from the dialogue later on\n",
    "# [word for (word, pos) in nltk.pos_tag(nltk.word_tokenize(' '.join([name.capitalize() for name in script.Name.dropna().unique()]))) if pos != 'NN']\n",
    "# nltk.pos_tag(nltk.word_tokenize(' '.join([name for name in script.Name.dropna().unique()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>single_name</th>\n",
       "      <th>count</th>\n",
       "      <th>in_double_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sam</td>\n",
       "      <td>399</td>\n",
       "      <td>sam tarly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>daario</td>\n",
       "      <td>166</td>\n",
       "      <td>daario naharis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>sandor</td>\n",
       "      <td>129</td>\n",
       "      <td>sandor clegane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>beric</td>\n",
       "      <td>92</td>\n",
       "      <td>beric dondarrion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guard</td>\n",
       "      <td>77</td>\n",
       "      <td>guard captain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>roose</td>\n",
       "      <td>77</td>\n",
       "      <td>roose bolton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>loras</td>\n",
       "      <td>75</td>\n",
       "      <td>loras tyrell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>barristan</td>\n",
       "      <td>67</td>\n",
       "      <td>barristan selmy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lancel</td>\n",
       "      <td>67</td>\n",
       "      <td>lancel lannister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>walder</td>\n",
       "      <td>55</td>\n",
       "      <td>walder frey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>walder</td>\n",
       "      <td>55</td>\n",
       "      <td>waldery frey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>renly</td>\n",
       "      <td>53</td>\n",
       "      <td>renly baratheon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>renly</td>\n",
       "      <td>53</td>\n",
       "      <td>renly dwarf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>janos</td>\n",
       "      <td>52</td>\n",
       "      <td>janos slunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>janos</td>\n",
       "      <td>52</td>\n",
       "      <td>janos slynt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>edmure</td>\n",
       "      <td>49</td>\n",
       "      <td>edmure roslin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>lyanna</td>\n",
       "      <td>45</td>\n",
       "      <td>lyanna mormont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>lysa</td>\n",
       "      <td>43</td>\n",
       "      <td>lysa arryn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>alliser</td>\n",
       "      <td>43</td>\n",
       "      <td>alliser thorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>alliser</td>\n",
       "      <td>43</td>\n",
       "      <td>alliser thorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>alliser</td>\n",
       "      <td>43</td>\n",
       "      <td>alliser throne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>balon</td>\n",
       "      <td>43</td>\n",
       "      <td>balon dwarf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>petyr</td>\n",
       "      <td>34</td>\n",
       "      <td>petyr baelish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>men</td>\n",
       "      <td>32</td>\n",
       "      <td>tommen lannister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>mace</td>\n",
       "      <td>32</td>\n",
       "      <td>mace tyrell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   single_name  count    in_double_name\n",
       "18         sam    399         sam tarly\n",
       "47      daario    166    daario naharis\n",
       "87      sandor    129    sandor clegane\n",
       "46       beric     92  beric dondarrion\n",
       "3        guard     77     guard captain\n",
       "37       roose     77      roose bolton\n",
       "25       loras     75      loras tyrell\n",
       "44   barristan     67   barristan selmy\n",
       "29      lancel     67  lancel lannister\n",
       "48      walder     55       walder frey\n",
       "49      walder     55      waldery frey\n",
       "26       renly     53   renly baratheon\n",
       "27       renly     53       renly dwarf\n",
       "12       janos     52       janos slunt\n",
       "11       janos     52       janos slynt\n",
       "45      edmure     49     edmure roslin\n",
       "88      lyanna     45    lyanna mormont\n",
       "55        lysa     43        lysa arryn\n",
       "57     alliser     43    alliser thorne\n",
       "58     alliser     43     alliser thorn\n",
       "59     alliser     43    alliser throne\n",
       "23       balon     43       balon dwarf\n",
       "71       petyr     34     petyr baelish\n",
       "28         men     32  tommen lannister\n",
       "51        mace     32       mace tyrell"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_names.sort_values(by='count', ascending=False).iloc[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nduplicate_names = pd.DataFrame(columns=['double_name', 'count', 'in_double_name'])\\nrow = 0\\nfor name in single_names:\\n    for d_name in double_names:\\n        if name in d_name. split(' ')[0]:\\n            count = len(script.Name[script.Name == name])\\n            duplicate_names.loc[row] = [name, count, d_name]\\n            row += 1\\n\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see if any in the double names could possibly be written wrong or be in the triple names?\n",
    "\"\"\"\n",
    "duplicate_names = pd.DataFrame(columns=['double_name', 'count', 'in_double_name'])\n",
    "row = 0\n",
    "for name in single_names:\n",
    "    for d_name in double_names:\n",
    "        if name in d_name. split(' ')[0]:\n",
    "            count = len(script.Name[script.Name == name])\n",
    "            duplicate_names.loc[row] = [name, count, d_name]\n",
    "            row += 1\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters 389\n",
      "Characters with link 368\n"
     ]
    }
   ],
   "source": [
    "# load dataset with all characters and extract json charactername to a dataframe\n",
    "with open('data/characters.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data['characters'])\n",
    "print(\"Characters\", df.shape[0])\n",
    "\n",
    "df = df[df['characterLink'].notnull()]\n",
    "print(\"Characters with link\", df.shape[0])\n",
    "characters = df.characterName\n",
    "\n",
    "l = []\n",
    "for name in np.unique(characters):\n",
    "    for n in name.split(' '):\n",
    "        l.append(n)\n",
    "unique_charnames = np.unique(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters that are found in the script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
